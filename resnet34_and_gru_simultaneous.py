# -*- coding: utf-8 -*-
"""resnet34_and_gru_simultaneous

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12VjmQ5SizGNT4ctMyOxTCDSfvAp43ZZK
"""

import json
import os
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import numpy as np
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")
print(f"Using device: {device}")

if device.type == "cuda":
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")

# Define ResNet model for feature extraction
class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT).to(device)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final FC layer

    def forward(self, x):
        x = self.resnet(x)
        return x.view(x.size(0), -1)  # Flatten the output

# GRU model with classification head
class GRUWithResNet(nn.Module):
    def __init__(self, feature_size, hidden_size, output_size, num_layers=3, dropout=0.3):
        super(GRUWithResNet, self).__init__()
        self.feature_extractor = ResNetFeatureExtractor()
        self.gru = nn.GRU(feature_size, hidden_size, num_layers=num_layers,
                          batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, output_size)
        )

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        x = x.view(-1, c, h, w)  # Combine batch and sequence for ResNet
        features = self.feature_extractor(x)
        features = features.view(batch_size, seq_len, -1)  # Reshape back to (batch, seq_len, feature_size)
        gru_out, _ = self.gru(features)
        out = self.fc(gru_out[:, -1, :])  # Use the last hidden state
        return out

# Dataset class for video clips
class VideoDataset(Dataset):
    def __init__(self, video_paths, labels, label_to_index, transform=None):
        self.video_paths = video_paths
        self.labels = labels
        self.label_to_index = label_to_index
        self.transform = transform

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]

        frames = []
        cap = cv2.VideoCapture(video_path)
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (224, 224))  # Resize to ResNet input size
            frame = frame / 255.0  # Normalize pixel values
            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        cap.release()

        frames = np.stack(frames)
        frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float()  # Convert to (seq_len, c, h, w)
        label_idx = self.label_to_index[label]

        return frames_tensor, label_idx

# Load dataset
def load_data(root_directory):
    video_paths = []
    labels = []
    for folder_name in os.listdir(root_directory):
        folder_path = os.path.join(root_directory, folder_name)
        if os.path.isdir(folder_path):
            label = folder_name
            video_files = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.avi', '.mov'))]
            for video_file in video_files:
                video_paths.append(os.path.join(folder_path, video_file))
                labels.append(label)
    return video_paths, labels

root_directory = r'/content/drive/MyDrive/extracted2.5sec - Use'
video_paths, labels = load_data(root_directory)

unique_labels = list(set(labels))
label_to_index = {label: idx for idx, label in enumerate(unique_labels)}
index_to_label = {idx: label for label, idx in label_to_index.items()}

# Split data into train and validation sets
train_paths, val_paths, train_labels, val_labels = train_test_split(
    video_paths, labels, test_size=0.2, random_state=42
)

# Hyperparameters
feature_size = 512  # ResNet output size
hidden_size = 512
output_size = len(unique_labels)
num_epochs = 20
batch_size = 2
learning_rate = 0.0001

# Create datasets and dataloaders
train_dataset = VideoDataset(train_paths, train_labels, label_to_index)
val_dataset = VideoDataset(val_paths, val_labels, label_to_index)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Instantiate model, loss function, and optimizer
model = GRUWithResNet(feature_size, hidden_size, output_size).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for video_batch, label_batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs}"):
        video_batch, label_batch = video_batch.to(device), label_batch.to(device)
        optimizer.zero_grad()
        outputs = model(video_batch)
        loss = criterion(outputs, label_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}, Training Loss: {avg_loss:.4f}")

    # Validation
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for video_batch, label_batch in val_loader:
            video_batch, label_batch = video_batch.to(device), label_batch.to(device)
            outputs = model(video_batch)
            predictions = torch.argmax(outputs, dim=1)
            correct += (predictions == label_batch).sum().item()
            total += label_batch.size(0)
    val_accuracy = correct / total
    print(f"Validation Accuracy: {val_accuracy:.4f}")

# Save model
torch.save(model.state_dict(), 'resnet_gru_highlight_model.pth')
print("Training completed and model saved.")

from google.colab import drive
drive.mount('/content/drive')

pip show torch

import json
import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch.nn as nn
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import confusion_matrix, classification_report

# Reuse the previous model classes and dataset class
class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final FC layer

    def forward(self, x):
        x = self.resnet(x)
        return x.view(x.size(0), -1)  # Flatten the output

class GRUWithResNet(nn.Module):
    def __init__(self, feature_size, hidden_size, output_size, num_layers=3, dropout=0.3):
        super(GRUWithResNet, self).__init__()
        self.feature_extractor = ResNetFeatureExtractor()
        self.gru = nn.GRU(feature_size, hidden_size, num_layers=num_layers,
                          batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, output_size)
        )

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        x = x.view(-1, c, h, w)  # Combine batch and sequence for ResNet
        features = self.feature_extractor(x)
        features = features.view(batch_size, seq_len, -1)  # Reshape back to (batch, seq_len, feature_size)
        gru_out, _ = self.gru(features)
        out = self.fc(gru_out[:, -1, :])  # Use the last hidden state
        return out

class VideoDataset(Dataset):
    def __init__(self, video_paths, labels, label_to_index, transform=None):
        self.video_paths = video_paths
        self.labels = labels
        self.label_to_index = label_to_index
        self.transform = transform

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]

        frames = []
        cap = cv2.VideoCapture(video_path)
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (224, 224))  # Resize to ResNet input size
            frame = frame / 255.0  # Normalize pixel values
            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        cap.release()

        frames = np.stack(frames)
        frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float()  # Convert to (seq_len, c, h, w)
        label_idx = self.label_to_index[label]

        return frames_tensor, label_idx, video_path

def load_data(root_directory):
    video_paths = []
    labels = []
    for folder_name in os.listdir(root_directory):
        folder_path = os.path.join(root_directory, folder_name)
        if os.path.isdir(folder_path):
            label = folder_name
            video_files = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.avi', '.mov'))]
            for video_file in video_files:
                video_paths.append(os.path.join(folder_path, video_file))
                labels.append(label)
    return video_paths, labels

def evaluate_model(model_path, test_directory):
    # Check if CUDA is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load video paths and labels
    test_paths, test_labels = load_data(test_directory)

    # Create label mapping
    unique_labels = list(set(test_labels))
    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}
    index_to_label = {idx: label for label, idx in label_to_index.items()}

    # Create test dataset and dataloader
    test_dataset = VideoDataset(test_paths, test_labels, label_to_index)
    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

    # Instantiate model
    model = GRUWithResNet(
        feature_size=512,
        hidden_size=512,
        output_size=len(unique_labels)
    ).to(device)

    # Load saved model weights
    model.load_state_dict(torch.load(model_path))
    model.eval()

    # Lists to store true labels and predictions
    true_labels = []
    predicted_labels = []
    video_details = []

    # Evaluation
    with torch.no_grad():
        correct = 0
        total = 0
        for video_batch, label_batch, video_paths in test_loader:
            video_batch, label_batch = video_batch.to(device), label_batch.to(device)
            outputs = model(video_batch)
            predictions = torch.argmax(outputs, dim=1)

            # Update tracking lists
            correct += (predictions == label_batch).sum().item()
            total += label_batch.size(0)

            true_labels.extend(label_batch.cpu().numpy())
            predicted_labels.extend(predictions.cpu().numpy())
            video_details.extend(video_paths)

    # Calculate overall accuracy
    accuracy = correct / total
    print(f"\nOverall Accuracy: {accuracy:.4f}")

    # Generate Classification Report
    print("\nClassification Report:")
    print(classification_report(
        true_labels,
        predicted_labels,
        target_names=[index_to_label[i] for i in range(len(index_to_label))]
    ))

    # Generate Confusion Matrix
    cm = confusion_matrix(true_labels, predicted_labels)
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=[index_to_label[i] for i in range(len(index_to_label))],
        yticklabels=[index_to_label[i] for i in range(len(index_to_label))]
    )
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.close()

    # Optional: Save misclassified videos information
    misclassified = []
    for i in range(len(true_labels)):
        if true_labels[i] != predicted_labels[i]:
            misclassified.append({
                'video_path': video_details[i],
                'true_label': index_to_label[true_labels[i]],
                'predicted_label': index_to_label[predicted_labels[i]]
            })

    with open('misclassified_videos.json', 'w') as f:
        json.dump(misclassified, f, indent=4)

    print("\nConfusion matrix saved as 'confusion_matrix.png'")
    print("Misclassified videos details saved as 'misclassified_videos.json'")

# Example usage
if __name__ == "__main__":
    model_path = '/content/resnet_gru_highlight_model.pth'
    test_directory = r'/content/drive/MyDrive/extracted2.5sec - Use'  # Update this to your test data directory

    evaluate_model(model_path, test_directory)

import json
import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch.nn as nn
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import confusion_matrix, classification_report

# Reuse the previous model classes and dataset class
class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final FC layer

    def forward(self, x):
        x = self.resnet(x)
        return x.view(x.size(0), -1)  # Flatten the output

class GRUWithResNet(nn.Module):
    def __init__(self, feature_size, hidden_size, output_size, num_layers=3, dropout=0.3):
        super(GRUWithResNet, self).__init__()
        self.feature_extractor = ResNetFeatureExtractor()
        self.gru = nn.GRU(feature_size, hidden_size, num_layers=num_layers,
                          batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, output_size)
        )

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        x = x.view(-1, c, h, w)  # Combine batch and sequence for ResNet
        features = self.feature_extractor(x)
        features = features.view(batch_size, seq_len, -1)  # Reshape back to (batch, seq_len, feature_size)
        gru_out, _ = self.gru(features)
        out = self.fc(gru_out[:, -1, :])  # Use the last hidden state
        return out

class VideoDataset(Dataset):
    def __init__(self, video_paths, labels, label_to_index, transform=None):
        self.video_paths = video_paths
        self.labels = labels
        self.label_to_index = label_to_index
        self.transform = transform

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]

        frames = []
        cap = cv2.VideoCapture(video_path)
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (224, 224))  # Resize to ResNet input size
            frame = frame / 255.0  # Normalize pixel values
            if self.transform:
                frame = self.transform(frame)
            frames.append(frame)
        cap.release()

        frames = np.stack(frames)
        frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float()  # Convert to (seq_len, c, h, w)
        label_idx = self.label_to_index[label]

        return frames_tensor, label_idx, video_path

def load_data(root_directory):
    video_paths = []
    labels = []
    for folder_name in os.listdir(root_directory):
        folder_path = os.path.join(root_directory, folder_name)
        if os.path.isdir(folder_path):
            label = folder_name
            video_files = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.avi', '.mov'))]
            for video_file in video_files:
                video_paths.append(os.path.join(folder_path, video_file))
                labels.append(label)
    return video_paths, labels

def evaluate_model(model_path, test_directory):
    # Check if CUDA is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load video paths and labels
    test_paths, test_labels = load_data(test_directory)

    # Create label mapping
    unique_labels = list(set(test_labels))
    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}
    index_to_label = {idx: label for label, idx in label_to_index.items()}

    # Create test dataset and dataloader
    test_dataset = VideoDataset(test_paths, test_labels, label_to_index)
    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

    # Instantiate model
    model = GRUWithResNet(
        feature_size=512,
        hidden_size=512,
        output_size=len(unique_labels)
    ).to(device)

    # Load saved model weights
    model.load_state_dict(torch.load(model_path))
    model.eval()

    # Lists to store true labels and predictions
    true_labels = []
    predicted_labels = []
    video_details = []

    # Evaluation
    with torch.no_grad():
        correct = 0
        total = 0
        for video_batch, label_batch, video_paths in test_loader:
            video_batch, label_batch = video_batch.to(device), label_batch.to(device)
            outputs = model(video_batch)
            predictions = torch.argmax(outputs, dim=1)

            # Update tracking lists
            correct += (predictions == label_batch).sum().item()
            total += label_batch.size(0)

            true_labels.extend(label_batch.cpu().numpy())
            predicted_labels.extend(predictions.cpu().numpy())
            video_details.extend(video_paths)

    # Calculate overall accuracy
    accuracy = correct / total
    print(f"\nOverall Accuracy: {accuracy:.4f}")

    # Generate Classification Report
    print("\nClassification Report:")
    print(classification_report(
        true_labels,
        predicted_labels,
        target_names=[index_to_label[i] for i in range(len(index_to_label))]
    ))

    # Generate Confusion Matrix
    cm = confusion_matrix(true_labels, predicted_labels)
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=[index_to_label[i] for i in range(len(index_to_label))],
        yticklabels=[index_to_label[i] for i in range(len(index_to_label))]
    )
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix_2.png')
    plt.close()

    # Optional: Save misclassified videos information
    misclassified = []
    for i in range(len(true_labels)):
        if true_labels[i] != predicted_labels[i]:
            misclassified.append({
                'video_path': video_details[i],
                'true_label': index_to_label[true_labels[i]],
                'predicted_label': index_to_label[predicted_labels[i]]
            })

    with open('misclassified_videos_2.json', 'w') as f:
        json.dump(misclassified, f, indent=4)

    print("\nConfusion matrix saved as 'confusion_matrix.png'")
    print("Misclassified videos details saved as 'misclassified_videos.json'")

# Example usage
if __name__ == "__main__":
    model_path = '/content/resnet_gru_highlight_model.pth'
    test_directory = r'/content/drive/MyDrive/2.5 - 2'  # Update this to your test data directory

    evaluate_model(model_path, test_directory)

import torch
import cv2
import numpy as np
import torchvision.models as models
import torch.nn as nn

# Redefine the model classes exactly as in the training script
class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final FC layer

    def forward(self, x):
        x = self.resnet(x)
        return x.view(x.size(0), -1)  # Flatten the output

class GRUWithResNet(nn.Module):
    def __init__(self, feature_size, hidden_size, output_size, num_layers=3, dropout=0.3):
        super(GRUWithResNet, self).__init__()
        self.feature_extractor = ResNetFeatureExtractor()
        self.gru = nn.GRU(feature_size, hidden_size, num_layers=num_layers,
                          batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, output_size)
        )

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        x = x.view(-1, c, h, w)  # Combine batch and sequence for ResNet
        features = self.feature_extractor(x)
        features = features.view(batch_size, seq_len, -1)  # Reshape back to (batch, seq_len, feature_size)
        gru_out, _ = self.gru(features)
        out = self.fc(gru_out[:, -1, :])  # Use the last hidden state
        return out

def predict_video(video_path, model, device, index_to_label):
    # Prepare the video frames
    frames = []
    cap = cv2.VideoCapture(video_path)
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, (224, 224))  # Resize to ResNet input size
        frame = frame / 255.0  # Normalize pixel values
        frames.append(frame)
    cap.release()

    # Convert frames to tensor
    frames = np.stack(frames)
    frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float().unsqueeze(0)  # Add batch dimension

    # Move to device and make prediction
    frames_tensor = frames_tensor.to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(frames_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        predicted_idx = torch.argmax(probabilities, dim=1).item()
        predicted_label = index_to_label[predicted_idx]
        confidence = probabilities[0][predicted_idx].item()

    return predicted_label, confidence

# Usage example
def main():
    # Configure these parameters based on your training setup
    feature_size = 512
    hidden_size = 512
    output_size = len(index_to_label)  # Number of unique labels

    # Select device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load the model
    model = GRUWithResNet(feature_size, hidden_size, output_size).to(device)
    model.load_state_dict(torch.load('/content/resnet_gru_highlight_model.pth'))

    # Path to the video you want to predict
    video_path = '/content/drive/MyDrive/2.5 - 2/Corner/Corner_half_1_pos_182775_idx_4.mp4'

    # Prediction
    predicted_label, confidence = predict_video(video_path, model, device, index_to_label)

    print(f"Predicted Label: {predicted_label}")
    print(f"Confidence: {confidence:.4f}")

if __name__ == "__main__":
    main()

import torch
import cv2
import numpy as np
import torchvision.models as models
import torch.nn as nn

# Redefine the model classes exactly as in the training script
class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final FC layer

    def forward(self, x):
        x = self.resnet(x)
        return x.view(x.size(0), -1)  # Flatten the output

class GRUWithResNet(nn.Module):
    def __init__(self, feature_size, hidden_size, output_size, num_layers=3, dropout=0.3):
        super(GRUWithResNet, self).__init__()
        self.feature_extractor = ResNetFeatureExtractor()
        self.gru = nn.GRU(feature_size, hidden_size, num_layers=num_layers,
                          batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, output_size)
        )

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        x = x.view(-1, c, h, w)  # Combine batch and sequence for ResNet
        features = self.feature_extractor(x)
        features = features.view(batch_size, seq_len, -1)  # Reshape back to (batch, seq_len, feature_size)
        gru_out, _ = self.gru(features)
        out = self.fc(gru_out[:, -1, :])  # Use the last hidden state
        return out

def predict_video(video_path, model, device, index_to_label):
    # Prepare the video frames
    frames = []
    cap = cv2.VideoCapture(video_path)
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, (224, 224))  # Resize to ResNet input size
        frame = frame / 255.0  # Normalize pixel values
        frames.append(frame)
    cap.release()

    # Convert frames to tensor
    frames = np.stack(frames)
    frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float().unsqueeze(0)  # Add batch dimension

    # Move to device and make prediction
    frames_tensor = frames_tensor.to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(frames_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        predicted_idx = torch.argmax(probabilities, dim=1).item()
        predicted_label = index_to_label[predicted_idx]
        confidence = probabilities[0][predicted_idx].item()

    return predicted_label, confidence

# Usage example
def main():
    # Configure these parameters based on your training setup
    feature_size = 512
    hidden_size = 512
    output_size = len(index_to_label)  # Number of unique labels

    # Select device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load the model
    model = GRUWithResNet(feature_size, hidden_size, output_size).to(device)
    model.load_state_dict(torch.load('/content/resnet_gru_highlight_model.pth'))

    # Path to the video you want to predict
    video_path = '/content/drive/MyDrive/2.5 - 2/Yellow card/Yellow card_half_2_pos_1021784_idx_132.mp4'

    # Prediction
    predicted_label, confidence = predict_video(video_path, model, device, index_to_label)

    print(f"Predicted Label: {predicted_label}")
    print(f"Confidence: {confidence:.4f}")

if __name__ == "__main__":
    main()

import torch
import cv2
import numpy as np
import torchvision.models as models
import torch.nn as nn

# Redefine the model classes exactly as in the training script
class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final FC layer

    def forward(self, x):
        x = self.resnet(x)
        return x.view(x.size(0), -1)  # Flatten the output

class GRUWithResNet(nn.Module):
    def __init__(self, feature_size, hidden_size, output_size, num_layers=3, dropout=0.3):
        super(GRUWithResNet, self).__init__()
        self.feature_extractor = ResNetFeatureExtractor()
        self.gru = nn.GRU(feature_size, hidden_size, num_layers=num_layers,
                          batch_first=True, dropout=dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, output_size)
        )

    def forward(self, x):
        batch_size, seq_len, c, h, w = x.size()
        x = x.view(-1, c, h, w)  # Combine batch and sequence for ResNet
        features = self.feature_extractor(x)
        features = features.view(batch_size, seq_len, -1)  # Reshape back to (batch, seq_len, feature_size)
        gru_out, _ = self.gru(features)
        out = self.fc(gru_out[:, -1, :])  # Use the last hidden state
        return out

def predict_video(video_path, model, device, index_to_label):
    # Prepare the video frames
    frames = []
    cap = cv2.VideoCapture(video_path)
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, (224, 224))  # Resize to ResNet input size
        frame = frame / 255.0  # Normalize pixel values
        frames.append(frame)
    cap.release()

    # Convert frames to tensor
    frames = np.stack(frames)
    frames_tensor = torch.tensor(frames).permute(0, 3, 1, 2).float().unsqueeze(0)  # Add batch dimension

    # Move to device and make prediction
    frames_tensor = frames_tensor.to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(frames_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        predicted_idx = torch.argmax(probabilities, dim=1).item()
        predicted_label = index_to_label[predicted_idx]
        confidence = probabilities[0][predicted_idx].item()

    return predicted_label, confidence

# Usage example
def main():
    # Configure these parameters based on your training setup
    feature_size = 512
    hidden_size = 512
    output_size = len(index_to_label)  # Number of unique labels

    # Select device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load the model
    model = GRUWithResNet(feature_size, hidden_size, output_size).to(device)
    model.load_state_dict(torch.load('/content/resnet_gru_highlight_model.pth'))

    # Path to the video you want to predict
    video_path = '/content/drive/MyDrive/nothing_half_2_pos_1869533_idx_161.mp4'

    # Prediction
    predicted_label, confidence = predict_video(video_path, model, device, index_to_label)

    print(f"Predicted Label: {predicted_label}")
    print(f"Confidence: {confidence:.4f}")

if __name__ == "__main__":
    main()

